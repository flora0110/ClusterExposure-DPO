training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 2
  warmup_steps: 20
  learning_rate: 2e-5
  num_train_epochs: 5
  logging_steps: 1
  bf16: true

lora:
  r: 16
  lora_alpha: 32
  target_modules: ['q_proj', 'v_proj']
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

base_model_name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
seed: 0

output_path: "/scratch/user/chuanhsin0110/ClusterExposure-DPO/experiments/SPRec_Reproduction/sft_model"

train_data_path: "/scratch/user/chuanhsin0110/ClusterExposure-DPO/data/sampled/Goodreads/train.json"
valid_data_path: "/scratch/user/chuanhsin0110/ClusterExposure-DPO/data/sampled/Goodreads/valid.json"

train_sample_size: 1024
valid_sample_size: 128
